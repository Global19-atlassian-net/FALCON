"""Obsolete.

We use a generic runner for lists of UOWs now.
"""
from __future__ import absolute_import
from __future__ import unicode_literals
import argparse
import collections
import glob
import logging
import os
import sys
import pypeflow.do_task
from .. import io
from .. import bash  # for write_script
from ..util import system

LOG = logging.getLogger()


TASK_DALIGNER_SCRIPT = """\
# Note: HPC.daligner chooses a merged filename in its generated script, so we will symlink to it.
python -m falcon_kit.mains.daligner --daligner-settings-fn={input.daligner_settings} --daligner-script-fn={input.daligner_script} --job-done-fn={output.job_done}
"""

# Here is some stuff basically copied from pypeflow.sample_tasks.py.
def validate(bash_template, inputs, outputs, parameterss):
    LOG.info('bash_script_from_template({}\n\tinputs={!r},\n\toutputs={!r})'.format(
        bash_template, inputs, outputs))
    def validate_dict(mydict):
        "Python identifiers are illegal as keys."
        try:
            collections.namedtuple('validate', mydict.keys())
        except ValueError as exc:
            LOG.exception('Bad key name in task definition dict {!r}'.format(mydict))
            raise
    validate_dict(inputs)
    validate_dict(outputs)
    validate_dict(parameterss)


def run(units_of_work_fn, las_paths_fn):
    uows = io.deserialize(units_of_work_fn)
    uow_dirs = list()
    las_paths = list()
    for i, uow in enumerate(uows):
        job = uow
        inputs = job['input']
        outputs = job['output'] # assumed to be relative to run-dir
        params = job['params']
        #params.update({k: v for (k, v) in viewitems(job['wildcards'])}) # include expanded wildcards
        LOG.warning('OUT:{}'.format(outputs))
        uow_dir = 'uow-{:02d}'.format(i)
        uow_dirs.append(uow_dir)
        io.mkdirs(uow_dir)
        with system.cd(uow_dir):
            pypeflow.do_task.run_bash(TASK_DALIGNER_SCRIPT, inputs, outputs, params)
            #job_done_fn = os.path.join(uow_dir, outputs['job_done'])
        #wildcards_str = '_'.join(w for w in itervalues(job['wildcards']))
        #job_name = 'job{}'.format(wildcards_str)
        #for (output_name, output_fn) in viewitems(outputs):
        #    giname = '{}_{}'.format(job_name, output_name)
        #    gather_inputs[giname] = output_fn
    for uow_dir in uow_dirs:
        # We could assert the existence of a job_done file here.
        d = os.path.abspath(uow_dir)
        las_path_glob = glob.glob(os.path.join(d, '*.las'))
        LOG.debug('dir={!r}, glob={!r}'.format(d, las_path_glob))
        las_paths.extend(las_path_glob)
    io.serialize(las_paths_fn, las_paths)


class HelpF(argparse.RawTextHelpFormatter, argparse.ArgumentDefaultsHelpFormatter):
    pass


def parse_args(argv):
    description = 'Run daligner.py once for each unit-of-work. Each will use a portion of the script generated by HPC.daligner.'
    epilog = 'The real .las output will be next to each job-done sentinel file. (We would have to parse HPC.daligner to learn the actual filename.) For now, runs will be in series, since we do not know how many processors we can use.'
    parser = argparse.ArgumentParser(
        description=description,
        epilog=epilog,
        formatter_class=HelpF,
    )
    parser.add_argument(
        '--units-of-work-fn',
        help='Input. JSON list of whatever we need for each run of daligner.py.',
    )
    parser.add_argument(
        '--las-paths-fn',
        help='Output. JSON list of .las files (sorted and first-level merged).',
    )
    args = parser.parse_args(argv[1:])
    return args


def main(argv=sys.argv):
    args = parse_args(argv)
    logging.basicConfig(level=logging.INFO)
    run(**vars(args))


if __name__ == '__main__':  # pragma: no cover
    main()
